{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 0,\n",
       " 'CD': 1,\n",
       " 'DT': 2,\n",
       " 'EX': 3,\n",
       " 'FW': 4,\n",
       " 'IN': 5,\n",
       " 'JJ': 6,\n",
       " 'JJR': 7,\n",
       " 'JJS': 8,\n",
       " 'LS': 9,\n",
       " 'MD': 10,\n",
       " 'NN': 11,\n",
       " 'NNS': 12,\n",
       " 'NNP': 13,\n",
       " 'NNPS': 14,\n",
       " 'PDT': 15,\n",
       " 'POS': 16,\n",
       " 'PRP': 17,\n",
       " 'PRP$': 18,\n",
       " 'RB': 19,\n",
       " 'RBR': 20,\n",
       " 'RBS': 21,\n",
       " 'RP': 22,\n",
       " 'SYM': 23,\n",
       " 'TO': 24,\n",
       " 'UH': 25,\n",
       " 'VB': 26,\n",
       " 'VBD': 27,\n",
       " 'VBG': 28,\n",
       " 'VBN': 29,\n",
       " 'VBP': 30,\n",
       " 'VBZ': 31,\n",
       " 'WDT': 32,\n",
       " 'WP': 33,\n",
       " 'WP$': 34,\n",
       " 'WRB': 35}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_1=dict({'CC':[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'CD':[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'DT':[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'EX':[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'FW':[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'IN':[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'JJ':[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'JJR':[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'JJS':[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'LS':[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'MD':[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'NN':[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'NNS':[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'NNP':[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'NNPS':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'PDT':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'POS':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'PRP':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'PRP$':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'RB':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'RBR':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'RBS':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'RP':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'SYM':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'TO':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'UH':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "'VB':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "'VBD':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "'VBG':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "'VBN':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "'VBP':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "'VBZ':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "'WDT':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "'WP':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "'WP$':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "'WRB':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]}\n",
    "             )\n",
    "pos_tags=dict({'0':[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'1':[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'2':[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'3':[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'4':[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'5':[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'6':[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'7':[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'8':[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'9':[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'10':[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'11':[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'12':[0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'13':[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'14':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'15':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'16':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'17':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'18':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'19':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'20':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'21':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'22':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'23':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'24':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "'25':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "'26':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "'27':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "'28':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "'29':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "'30':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "'31':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "'32':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "'33':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "'34':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "'35':[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]}\n",
    "             )\n",
    "tags=['WRB','WP$','WP','WDT','VBZ','VBP','VBN','VBG','VBD','VB','UH','TO','SYM','RP','RBS','RBR','RB','PRP$','PRP','POS','PDT'\n",
    ",'NNPS','NNP','NNS','NN','MD','LS','JJS','JJR','JJ','IN','FW','EX','DT','CD','CC']\n",
    "# Reversing a list using reversed() \n",
    "def Reverse(lst): \n",
    "    return [ele for ele in reversed(lst)]\n",
    "tags=Reverse(tags)\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename, encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    " \n",
    "def get_weight(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = 37\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix_1 = np.zeros((vocab_size, 36))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix_1[i] = vector\n",
    "    return weight_matrix_1\n",
    "# load the vocabulary\n",
    "vocab_filename = 'Movie_review/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('Movie_review/txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs('Movie_review/txt_sentoken/neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP as scn\n",
    "nlp = scn(r'stanford_parser\\stanford-corenlp-full-2018-10-05/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence):\n",
    "    t=nlp.pos_tag(sentence)\n",
    "    list1=[]\n",
    "    for i in t:\n",
    "        tag=i[1]\n",
    "        list1.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in nlp.pos_tag(sentence)] for sentence in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    1800.000000\n",
       "mean      340.327222\n",
       "std       147.835862\n",
       "min         7.000000\n",
       "25%       240.000000\n",
       "50%       320.500000\n",
       "75%       413.000000\n",
       "max      1317.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "reviews_len = [len(x) for x in encoded_docs]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = pad_sequences(encoded_docs, maxlen=400, padding='post',truncating='post',value=0)\n",
    "Xtrain_1=pad_sequences(y, maxlen=400, padding='post',truncating='post',value=0)\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    " \n",
    "# load all test reviews\n",
    "positive_docs = process_docs('Movie_review/txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('Movie_review/txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "y_1 = [[tag2idx[w[1]] for w in nlp.pos_tag(sentence)] for sentence in test_docs]\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=400, padding='post',truncating='post',value=0)\n",
    "Xtest_1=pad_sequences(y_1, maxlen=400, padding='post',truncating='post',value=0)\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.6B.100d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "embedding_pos=get_weight(pos_tags_1,tag2idx)# create the embedding layer\n",
    "#embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "embedding_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.19915999, -0.049702  ,  0.24579   , ..., -0.068109  ,\n",
       "         0.017651  ,  0.06455   ],\n",
       "       [-0.22556999,  0.49417999,  0.48609999, ..., -0.45743999,\n",
       "         0.49645999,  0.34906   ],\n",
       "       ...,\n",
       "       [-0.048561  ,  0.23923001,  0.57498002, ..., -0.0028215 ,\n",
       "         0.44571999,  0.34439999],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix=torch.from_numpy(embedding_vectors)\n",
    "weight_matrix_1=torch.from_numpy(embedding_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings=25768\n",
    "    embedding_dim= 100\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer_pos(weights_matrix_1, non_trainable=False):\n",
    "    num_embeddings_pos=37\n",
    "    embedding_dim_pos= 36\n",
    "    emb_layer_pos = nn.Embedding(num_embeddings_pos, embedding_dim_pos)\n",
    "    emb_layer_pos.load_state_dict({'weight': weights_matrix_1})\n",
    "    if non_trainable:\n",
    "        emb_layer_pos.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer_pos, num_embeddings_pos, embedding_dim_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, weight_matrix,weight_matrix_1, hidden_size, num_layers):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.embedding, self.num_embeddings, self.embedding_dim = create_emb_layer(weight_matrix, True)\n",
    "        self.embedding_pos, self.num_embeddings_pos, self.embedding_dim_pos = create_emb_layer_pos(weight_matrix_1, True)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=136, hidden_size=512, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=512, out_features=1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "       \n",
    "    def forward(self, inp,inp_1, hidden):\n",
    "        embeds_out = self.embedding(inp)\n",
    "        embeds_out_1=self.embedding_pos(inp_1)\n",
    "        out_1=torch.cat((embeds_out,embeds_out_1),2)\n",
    "        lstm_out, h = self.lstm(out_1, hidden)\n",
    "        fc_out = self.fc(lstm_out.contiguous().view(-1, 512))\n",
    "        sigm_out = self.sigm(fc_out)\n",
    "        output=sigm_out\n",
    "        #output = sigm_out.view(30, -1)\n",
    "        #output=output[:,-1]\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return None\n",
    "    \n",
    "lstm_model = LSTM(weight_matrix,weight_matrix_1, 512, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(Xtrain), torch.from_numpy(ytrain))\n",
    "test_data = TensorDataset(torch.from_numpy(Xtest), torch.from_numpy(ytest))\n",
    "train_data_1 = TensorDataset(torch.from_numpy(Xtrain_1), torch.from_numpy(ytrain))\n",
    "test_data_1 = TensorDataset(torch.from_numpy(Xtest_1), torch.from_numpy(ytest))\n",
    "# dataloaders\n",
    "batch_size = 20\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "train_loader_1 = DataLoader(train_data_1, shuffle=True, batch_size=batch_size)\n",
    "test_loader_1 = DataLoader(test_data_1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: -2.573\n",
      "[1,    10] loss: -2.592\n",
      "[1,    15] loss: -2.610\n",
      "[1,    20] loss: -2.618\n",
      "[1,    25] loss: -2.623\n",
      "[1,    30] loss: -2.635\n",
      "[1,    35] loss: -2.653\n",
      "[1,    40] loss: -2.664\n",
      "[1,    45] loss: -2.680\n",
      "[1,    50] loss: -2.683\n",
      "[1,    55] loss: -2.699\n",
      "[1,    60] loss: -2.713\n",
      "[1,    65] loss: -2.721\n",
      "[1,    70] loss: -2.741\n",
      "[1,    75] loss: -2.746\n",
      "[1,    80] loss: -2.761\n",
      "[1,    85] loss: -2.768\n",
      "[1,    90] loss: -2.784\n",
      "[2,     5] loss: -2.793\n",
      "[2,    10] loss: -2.802\n",
      "[2,    15] loss: -2.810\n",
      "[2,    20] loss: -2.827\n",
      "[2,    25] loss: -2.850\n",
      "[2,    30] loss: -2.855\n",
      "[2,    35] loss: -2.872\n",
      "[2,    40] loss: -2.890\n",
      "[2,    45] loss: -2.889\n",
      "[2,    50] loss: -2.904\n",
      "[2,    55] loss: -2.901\n",
      "[2,    60] loss: -2.946\n",
      "[2,    65] loss: -2.936\n",
      "[2,    70] loss: -2.955\n",
      "[2,    75] loss: -2.951\n",
      "[2,    80] loss: -2.976\n",
      "[2,    85] loss: -2.985\n",
      "[2,    90] loss: -3.004\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=0.005)\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader,train_loader_1), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs=data[0][0]\n",
    "        labels = data[0][1]\n",
    "        inputs_1=data[1][0]\n",
    "        labels_1=data[1][1]\n",
    "        hidden = lstm_model.init_hidden()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        lstm_model.zero_grad()\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        inputs_1 = inputs_1.type(torch.LongTensor)\n",
    "        output=lstm_model(inputs,inputs_1,hidden)\n",
    "        output = output.view(20, -1)\n",
    "        # forward + backward + optimize\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 5 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 54 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader,test_loader_1):\n",
    "        inputs=data[0][0]\n",
    "        labels = data[0][1]\n",
    "        inputs_1=data[1][0]\n",
    "        labels_1=data[1][1]\n",
    "        hidden = lstm_model.init_hidden()\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        inputs_1 = inputs_1.type(torch.LongTensor)\n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        output = lstm_model(inputs,inputs_1,hidden)\n",
    "        output = output.view(20, -1)\n",
    "        output=output[:,-1]\n",
    "        #print(output)\n",
    "        #print(output.shape)\n",
    "        for i,j in enumerate(output):\n",
    "            if j>0.5745:\n",
    "                predicted=1\n",
    "            else:\n",
    "                predicted=0\n",
    "            total += 1\n",
    "            if(predicted==labels[i]):\n",
    "                correct += 1\n",
    "\n",
    "print('Accuracy of the network on the test set: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
